{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hTedkHtzdJ4y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(text_file, n_words):\n",
        "    with open(text_file, 'r') as f:\n",
        "        text = f.read()\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([text])\n",
        "    encoded_text = tokenizer.texts_to_sequences([text])[0]\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    X, y = [], []\n",
        "    for i in range(n_words, len(encoded_text)):\n",
        "        X.append(encoded_text[i-n_words:i])\n",
        "        y.append(encoded_text[i])\n",
        "    X = np.array(X)\n",
        "    y = to_categorical(y, num_classes=vocab_size)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 50, input_length=n_words))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    model.fit(X, y, epochs=100)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "iWOD0beoeQJy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_word(model, tokenizer, input_text, n_words):\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    input_sequence = np.array([input_sequence[-n_words:]])\n",
        "    pred_word_prob = model.predict(input_sequence, verbose=0)[0]\n",
        "    pred_word_index = np.argmax(pred_word_prob)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == pred_word_index:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "TTaxYgixeX35"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = 'dataset.txt'\n",
        "n_words = 3\n",
        "model, tokenizer = predict_next_word(text_file, n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB__5HKWeb80",
        "outputId": "f37bf862-a87d-43d3-9c83-375a90082863"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "468/468 [==============================] - 12s 14ms/step - loss: 6.4726\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 6.0593\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 5.9622\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 5.8609\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 5.7051\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 5.5187\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 5.3413\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 5.1699\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 4.9921\n",
            "Epoch 10/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 4.8157\n",
            "Epoch 11/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 4.6459\n",
            "Epoch 12/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 4.4802\n",
            "Epoch 13/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 4.3183\n",
            "Epoch 14/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 4.1601\n",
            "Epoch 15/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 4.0053\n",
            "Epoch 16/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 3.8538\n",
            "Epoch 17/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 3.7063\n",
            "Epoch 18/100\n",
            "468/468 [==============================] - 4s 10ms/step - loss: 3.5628\n",
            "Epoch 19/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 3.4235\n",
            "Epoch 20/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 3.2863\n",
            "Epoch 21/100\n",
            "468/468 [==============================] - 4s 9ms/step - loss: 3.1546\n",
            "Epoch 22/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 3.0268\n",
            "Epoch 23/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 2.9044\n",
            "Epoch 24/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 2.7857\n",
            "Epoch 25/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 2.6712\n",
            "Epoch 26/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 2.5631\n",
            "Epoch 27/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 2.4590\n",
            "Epoch 28/100\n",
            "468/468 [==============================] - 4s 10ms/step - loss: 2.3590\n",
            "Epoch 29/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 2.2609\n",
            "Epoch 30/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 2.1670\n",
            "Epoch 31/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 2.0765\n",
            "Epoch 32/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 1.9900\n",
            "Epoch 33/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.9050\n",
            "Epoch 34/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 1.8239\n",
            "Epoch 35/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.7476\n",
            "Epoch 36/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.6734\n",
            "Epoch 37/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 1.6010\n",
            "Epoch 38/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.5313\n",
            "Epoch 39/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 1.4643\n",
            "Epoch 40/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.4036\n",
            "Epoch 41/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.3417\n",
            "Epoch 42/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 1.2842\n",
            "Epoch 43/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.2273\n",
            "Epoch 44/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 1.1749\n",
            "Epoch 45/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.1221\n",
            "Epoch 46/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 1.0742\n",
            "Epoch 47/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 1.0259\n",
            "Epoch 48/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.9811\n",
            "Epoch 49/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.9399\n",
            "Epoch 50/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.8970\n",
            "Epoch 51/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.8568\n",
            "Epoch 52/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.8194\n",
            "Epoch 53/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.7848\n",
            "Epoch 54/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.7486\n",
            "Epoch 55/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.7168\n",
            "Epoch 56/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.6850\n",
            "Epoch 57/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.6562\n",
            "Epoch 58/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.6283\n",
            "Epoch 59/100\n",
            "468/468 [==============================] - 5s 12ms/step - loss: 0.6014\n",
            "Epoch 60/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.5750\n",
            "Epoch 61/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.5519\n",
            "Epoch 62/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.5292\n",
            "Epoch 63/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.5050\n",
            "Epoch 64/100\n",
            "468/468 [==============================] - 5s 12ms/step - loss: 0.4854\n",
            "Epoch 65/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.4672\n",
            "Epoch 66/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.4484\n",
            "Epoch 67/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 0.4304\n",
            "Epoch 68/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.4135\n",
            "Epoch 69/100\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.3981\n",
            "Epoch 70/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.3857\n",
            "Epoch 71/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.3688\n",
            "Epoch 72/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.3572\n",
            "Epoch 73/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.3443\n",
            "Epoch 74/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 0.3345\n",
            "Epoch 75/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.3222\n",
            "Epoch 76/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.3114\n",
            "Epoch 77/100\n",
            "468/468 [==============================] - 5s 12ms/step - loss: 0.3054\n",
            "Epoch 78/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2946\n",
            "Epoch 79/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.2850\n",
            "Epoch 80/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2780\n",
            "Epoch 81/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.2711\n",
            "Epoch 82/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.2640\n",
            "Epoch 83/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2580\n",
            "Epoch 84/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 0.2530\n",
            "Epoch 85/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2467\n",
            "Epoch 86/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.2418\n",
            "Epoch 87/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.2366\n",
            "Epoch 88/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2333\n",
            "Epoch 89/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 0.2302\n",
            "Epoch 90/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2231\n",
            "Epoch 91/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.2213\n",
            "Epoch 92/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.2167\n",
            "Epoch 93/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2146\n",
            "Epoch 94/100\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 0.2113\n",
            "Epoch 95/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2076\n",
            "Epoch 96/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.2047\n",
            "Epoch 97/100\n",
            "468/468 [==============================] - 5s 11ms/step - loss: 0.2030\n",
            "Epoch 98/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.2007\n",
            "Epoch 99/100\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 0.1990\n",
            "Epoch 100/100\n",
            "468/468 [==============================] - 5s 10ms/step - loss: 0.1958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Enter a word or phrase to predict the next word:\")\n",
        "print(\"Next word: \",next_word(model, tokenizer, input_text,n_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRT27Pnhge_d",
        "outputId": "aa4ebb84-4269-49a1-d47e-b68f2b519e95"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word or phrase to predict the next word:This dramatic\n",
            "Next word:  rendering\n"
          ]
        }
      ]
    }
  ]
}